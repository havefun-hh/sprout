# Faster R-CNN 论文笔记

### 摘要

​		最新的检测网络都依赖区域推荐算法来推测物体位置。像SPPnet[1]和Fast R-CNN[2]已经大幅削减了检测网络的时间开销，但**像卷积特征，使得区域推荐的开销几近为0**。**一个RPN是一个全卷积网络技能预测物体的边框，同时也能对该位置进行物体打分**。RPN通过端到端的训练可以产生高质量的推荐区域，然后再用Fast R-CNN进行检测。通过共享卷积特征，我们进一步整合RPN和Fast R-CNN到一个网络，用近期流行的“术语”说，就是一种“注意力”机制。RPN组件会告诉整合网络去看哪个部分。对于非常深的VGG-16模型[3]。我们的检测系统在GPU上达到了5fps的检测帧率（包括所有步骤），同时也在PASCAL VOC2007,2012和MS COCO数据集上达到了最好的物体检测精度，而对每张图片只推荐了300个区域。在ILSVRC和COCO 2015竞赛中，Faster R-CNN和RPN是多个赛道都赢得冠军的基础。代码已经公开。

### 1. 介绍

​		区域推荐方法(比如[4])和基于区域的卷及神经网络（RCNNs）[5]的成功推动了物体检测水平的进步。尽管RCNNs刚开发出来时[5]十分费时，经过[1],[2]的跨推荐区域的共享卷积的改进，已经大幅消减了开销。近期大作Fast R-CNN[2]，如果不考虑区域推荐的耗时，使用超深度网络[3]已经达到几乎实时的处理速度。但**推荐显然是最先进检测系统的瓶颈**。

​		区域推荐算法主要依赖简单的特征和经济的推理机制。最受欢迎的方法——选择性搜索[4]是基于低层次的人工特征贪婪地进行超级像素合并。而跟有效的检测网络[2]相比，选择性搜索的就慢了一个数量级，CPU上每张图片耗时2秒。EdgeBoxes[6]当前做到了速度和推荐质量的最佳平衡。然而，在整个检测网络中，区域推荐这一步仍然是主要耗时阶段。

​		你也许会注意到快速的基于推荐的CNNs充分利用了GPU，而区域推荐算法都是CPU中实现的。所以进行这个时间比较是不公平的。如果想加速它，用GPU实现就好了呀。这也许是个有效的工程化解决方案，但重新实现仍然会忽略下游的检测网络，因而也失去了共享计算的好机会。
​		本文将向您展示一个算法上的改变——**使用深度卷积神经网络计算推荐区域**——将引出一个优雅而高效的解决方案，在给定检测网络完成的计算的基础上，让区域的计算近乎为0。鉴于此，我们向大家隆重介绍这个**新型的区域推荐网络（Region Proposal Networks，RPNs），它和当今世界最棒的检测网络[1],[2]共享卷积层。通过在测试阶段共享卷积，让计算推荐区域的边际成本变得很低**（比如每张图片10ms）。

![Faster R-CNN 图1](E:\Typora\论文笔记\Faster R-CNN 图1.png)

图1：解决多尺度和尺寸的不同方案。（a）构建图像和特征映射金字塔，分类器以各种尺度运行。（b）在特征映射上运行具有多个比例/大小的滤波器的金字塔。（c）我们在回归函数中使用参考边界框金字塔。

​		RPN旨在有效预测具有广泛尺度和长宽比的区域提议。与使用图像金字塔（图1，a）或滤波器金字塔（图1，b）的流行方法[8]，[9]，[1]相比，我们引入新的“锚”盒作为多种尺度和长宽比的参考。我们的方案可以被认为是回归参考金字塔（图1，c），它避免了枚举多种比例或长宽比的图像或滤波器。这个模型在使用单尺度图像进行训练和测试时运行良好，从而有利于运行速度。

​		我们观察到像Fast R-CNN这样的基于区域的检测器锁使用的卷积特征图也可以用来生成推荐区域。在这些卷积层的特征之上，我们通过添加一些额外的卷积网络引入一个RPN，可以和回归约束框、物体打分相并列。RPN是一种完全卷积网络(FCN)[7]，可以为特定任务进行端到端的训练来产生检测推荐。

​		为了统一RPNs和Fast R-CNN[2]物体检测网络，我们提出一种介于区域推荐任务调优和之后的物体检测调优之间的训练方法，同时还能保证固定的推荐。这个方法可以很快收敛，并产生一个统一的网络，该网络在两个任务上共享卷积特征。

​		我们在PASCAL VOC检测benchmarks[11]上全面评估了我们的方法，RPNs结合Fast R-CNNs可以比选择性搜索结合Fast R-CNN有更高的准确度。于此同时我们的方法摒弃了选择性搜索在测试阶段几乎所有的计算负担，有效推荐的运行时间只有区区的10毫秒。使用十分耗时的超深度模型[3]，我们的检测方法仍然可以在GPU上达到5fps的速度，这使得物体检测系统在速度和精度上都变得更加使用。我们也报告了在MS COCO数据集[12]上的结果，探究了PASCAL VOS上使用COCO数据集带来的提升。代码现在开放在 https://github.com/shaoqingren/faster_rcnn (in MATLAB)和https://github.com/rbgirshick/py-faster-rcnn (in Python)。

​		本文的一个早期版本发布在[10]上。从那时起，RPN和Faster R-CNN的框架就已经被采用，并应用到其他的方法中，比如3D物体检测[13]，基于组件的检测[14]，实力分割[[13]和图像字幕[16]。我们的快速而有效的物体检测系统已经构建在想Pinterests[17]这样的商业系统中，提升了用户交互。

​			在ILSVRC和COCO 2015竞赛中，Faster R-CNN和RPN是多项分赛长的第一名[18]，包括ImageNet 检测，ImageNet定位，COCO检测和COCO分割。RPNs从数据中完全学会了推荐区域，而且使用更深或更有表达力的特征（比如101层的Resnet[18]）效果会更好。Faster R-CNN和RPN也用于多个其他领先名词的团队所使用。这些结果都说明我们的方法不仅实用省时，而且有效精准。

### 2. 相关工作

​		**物体推荐**。有大量的推荐方法。有一些综述和这些方法的比较可见于[19], [20], [21]。广泛使用的方法很多基于grouping super-pixels (比如,Selective Search [4], CPMC [22], MCG [23])，还有一些基于滑动窗口(e.g., 比如窗口的物体属性objectness in windows[24], EdgeBoxes [6])。物体推荐方法也经常独立于它的检测器而被很多外部的模块使用 (比如, Selective Search [4] object detectors, RCNN [5], 和Fast R-CNN [2])。

​		**用于物体检测的深度网络**。R-CNN方法[5]端到端地训练CNNs，用于将推荐区域分类成物体类别或背景。**R-CNN主要扮演了分类器的角色，它并不预测物体的边框（除了用于约束框回归的净化模块）。他的精度依赖于区域推荐模块的性能**（见[20]中的比较）。多篇论文推荐是用深度网络预测物体约束框 [25], [9], [26], [27]。OverFeat方法中，一个全连接网络用于训练预测定位任务的单一物体的框坐标。为了检测多个特定类的物体又将全连接层转变成卷积层。MultiBox方法[26],[27]也使用网络产生推荐，它的最后一个全连接层可以同时预测多个未知类的框，推广了OverFeat的“单框”风格。这些未知类方框也被R-CNN[5]所使用。MuiltiBox推荐网络应用于单张图片的一个裁切，或者一个大型图片的多个裁切（比如224×224），和我们的全卷积模式完全不同。MultiBox并不在推荐和检测网络之间共享特征。后面结合我们的方法，我们将深入讨论OverFeat和MultiBox。和我们的工作同时进行的DeepMask方法[28]也被开发出来用于语义推荐。

​		卷积[9]，[1]，[29]，[7]，[2]的共享计算已经越来越受到人们的关注，因为它可以有效而准确地进行视觉识别。OverFeat论文[9]计算图像金字塔的卷积特征用于分类，定位和检测。共享卷积特征映射的自适应大小池化（SPP）[1]被开发用于有效的基于区域的目标检测[1]，[30]和语义分割[29]。Fast R-CNN[2]能够对共享卷积特征进行端到端的检测器训练，并显示出令人信服的准确性和速度。

### 3. FASTER R-CNN

​		我们的目标检测系统，称为Faster R-CNN，**由两个模块组成**。**第一个模块是提议区域的深度全卷积网络**，**第二个模块是使用提议区域的Fast R-CNN检测器**[2]。整个系统是一个单个的，统一的目标检测网络（图2）。使用最近流行的“注意力”[31]机制的神经网络术语，RPN模块告诉Fast R-CNN模块在哪里寻找。在第3.1节中，我们介绍了区域提议网络的设计和属性。在第3.2节中，我们开发了用于训练具有共享特征模块的算法。

![](E:\Typora\论文笔记\Faster R-CNN 图2.png)

#### 3.1 区域提议网络

​		区域提议网络（RPN）以任意大小的图像作为输入，输出一组矩形的目标提议，每个提议都有一个目标得分。我们用全卷积网络[7]对这个过程进行建模，我们将在本节进行描述。因为我们的最终目标是与Fast R-CNN目标检测网络[2]共享计算，所以我们假设两个网络共享一组共同的卷积层。在我们的实验中，我们研究了具有5个共享卷积层的Zeiler和Fergus模型[32]（ZF）和具有13个共享卷积层的Simonyan和Zisserman模型[3]（VGG-16）。

![Faster R-CNN 图3](E:\Typora\论文笔记\Faster R-CNN 图3.png)

图3：左：区域提议网络（RPN）。右：在PASCAL VOC 2007测试集上使用RPN提议的示例检测。我们的方法可以检测各种尺度和长宽比的目标。

​		为了生成区域提议，我们在最后的共享卷积层输出的卷积特征映射上滑动一个小网络。这个小网络将输入卷积特征映射的n×n空间窗口作为输入。每个滑动窗口映射到一个低维特征（ZF为256维，VGG为512维，后面是ReLU[33]）。这个特征被输入到两个子全连接层——一个边界框回归层（reg）和一个边界框分类层（cls）。在本文中，我们使用 $n=3$，注意输入图像上的有效感受野是大的（ZF和VGG分别为171和228个像素）。图3（左）显示了这个小型网络的一个位置。请注意，因为小网络以滑动窗口方式运行，所有空间位置共享全连接层。这种架构通过一个n×n卷积层，后面是两个子1×1卷积层（分别用于reg和cls）自然地实现。

##### 3.1.1 锚点

​		在每个滑动窗口位置，我们同时预测多个区域提议，其中每个位置可能提议的最大数目表示为 $k$ 。因此，reg层具有 $4k$ 个输出，编码 $k$ 个边界框的坐标，cls层输出 $2k$ 个分数，估计每个提议是目标或不是目标的概率。相对于我们称之为锚点的 $k$ 个参考边界框，$k$ 个提议是参数化的。锚点位于所讨论的滑动窗口的中心，并与一个尺度和长宽比相关（图3左）。默认情况下，我们使用3个尺度和3个长宽比，在每个滑动位置产生 $k=9$ 个锚点。对于大小为W×H（通常约为2400）的卷积特征映射，总共有 $WHk$ 个锚点。

**平移不变的锚点**

在每个滑动窗口位置，我们同时预测多个区域提议，其中每个位置可能提议的最大数目表示为kk。因此，*reg*层具有4k4k个输出，编码kk个边界框的坐标，*cls*层输出2k2k个分数，估计每个提议是目标或不是目标的概率。相对于我们称之为锚点的kk个参考边界框，kk个提议是参数化的。锚点位于所讨论的滑动窗口的中心，并与一个尺度和长宽比相关（图3左）。默认情况下，我们使用3个尺度和3个长宽比，在每个滑动位置产生k=9k=9个锚点。对于大小为W×H（通常约为2400）的卷积特征映射，总共有WHkWHk个锚点。

​		我们的方法的一个重要特性是它是**平移不变**的，无论是在锚点还是计算相对于锚点的区域提议的函数。如果在图像中平移目标，提议应该平移，并且同样的函数应该能够在任一位置预测提议。平移不变特性是由我们的方法保证的。作为比较，MultiBox方法[27]使用k-means生成800个锚点，这不是平移不变的。所以如果平移目标，MultiBox不保证会生成相同的提议。

​		平移不变特性也减小了模型的大小。MultiBox有 $(4+1)×800$ 维的全连接输出层，而我们的方法在 $k=9$ 个锚点的情况下有 $(4+2)×9$ 维的卷积输出层。因此，对于VGG-16，我们的输出层具有 $2.8×104$ 个参数（对于VGG-16为 $512×(4+2)×9$ ，比MultiBox输出层的 $6.1×106$ 个参数少了两个数量级（对于MultiBox [27]中的GoogleNet[34]为 $1536×(4+1)×800$ ）。如果考虑到特征投影层，我们的提议层仍然比MultiBox少一个数量级。我们期望我们的方法在PASCAL VOC等小数据集上有更小的过拟合风险。

**多尺度锚点作为回归参考**

​		我们的锚点设计提出了一个新的方案来解决多尺度（和长宽比）。如图1所示，多尺度预测有两种流行的方法。第一种方法是基于图像/特征金字塔，例如DPM[8]和基于CNN的方法[9]，[1]，[2]中。图像在多个尺度上进行缩放，并且针对每个尺度（图1（a））计算特征映射（HOG[8]或深卷积特征[9]，[1]，[2]）。这种方法通常是有用的，但是非常耗时。第二种方法是在特征映射上使用多尺度（和/或长宽比）的滑动窗口。例如，在DPM[8]中，使用不同的滤波器大小（例如5×7和7×5）分别对不同长宽比的模型进行训练。如果用这种方法来解决多尺度问题，可以把它看作是一个“滤波器金字塔”（图1（b））。第二种方法通常与第一种方法联合采用[8]。

​		作为比较，我们的基于锚点方法建立在锚点金字塔上，这是更具成本效益的。我们的方法参照多尺度和长宽比的锚盒来分类和回归边界框。它只依赖单一尺度的图像和特征映射，并使用单一尺寸的滤波器（特征映射上的滑动窗口）。我们通过实验来展示这个方案解决多尺度和尺寸的效果（表8）。

![Faster R-CNN 表8](E:\Typora\论文笔记\Faster R-CNN 表8.png)



表8：Faster R-CNN在PAS-CAL VOC 2007测试数据集上使用不同锚点设置的检测结果。网络是VGG-16。训练数据是VOC 2007训练集。使用3个尺度和3个长宽比（$69.9%$）的默认设置，与表3中的相同。

​		由于这种基于锚点的多尺度设计，我们可以简单地使用在单尺度图像上计算的卷积特征，Fast R-CNN检测器也是这样做的[2]。多尺度锚点设计是共享特征的关键组件，不需要额外的成本来处理尺度。

##### 3.1.2 损失函数

​		为了训练RPN，我们为每个锚点分配一个二值类别标签（是目标或不是目标）。我们给两种锚点分配一个正标签：（i）具有与实际边界框的重叠最高交并比（IoU）的锚点，或者（ii）具有与实际边界框的重叠超过0.7 IoU的锚点。注意，单个真实边界框可以为多个锚点分配正标签。通常第二个条件足以确定正样本；但我们仍然采用第一个条件，因为在一些极少数情况下，第二个条件可能找不到正样本。对于所有的真实边界框，如果一个锚点的IoU比率低于0.3，我们给非正面的锚点分配一个负标签。既不正面也不负面的锚点不会有助于训练目标函数。

​		根据这些定义，我们对目标函数Fast R-CNN[2]中的多任务损失进行最小化。我们对图像的损失函数定义为：
$$
L(\{p_i\},\{t_i\})=\frac{1}{N_{cls}}\sum_{i}{L_{cls}}(p_i,p^*_i)+λ\frac{1}{N_{reg}}\sum_{i}{p^*_i}L_{reg}(t_i,t^*_i). \qquad(1)
$$
其中，$i$ 是一个小批量数据中锚点的索引，$p_i$ 是锚点ii作为目标的预测概率。如果锚点为正，真实标签 $p^*_i$ 为1，如果锚点为负，则为0。$t_i$ 是表示预测边界框4个参数化坐标的向量，而 $t^*_i$ 是与正锚点相关的真实边界框的向量。分类损失 $L_{cls}$ 是两个类别上（目标或不是目标）的对数损失。对于回归损失，我们使用 $L_{reg}(t_i,t^*_i)=R(t_i−t^*_i)$ ，其中 $R$ 是在[2]中定义的鲁棒损失函数（平滑 $L1$ ）。项 $p^*_iL_{reg}$ 表示回归损失仅对于正锚点激活，否则被禁用（$p^*_i=0$）。*cls* 和 *reg* 层的输出分别由 ${p_i}$ 和 ${t_i}$ 组成。

​		这两个项用 $N_{cls}$ 和 $N_{reg}$ 进行标准化，并由一个平衡参数 λ 加权。在我们目前的实现中（如在发布的代码中），方程（1）中的 $cls$ 项通过小批量数据的大小（即$N_{cls}=256$）进行归一化，$reg$ 项根据锚点位置的数量（即，$N_{reg}∼24000$）进行归一化。默认情况下，我们设置 $λ=10 $，因此 *cls* 和 *reg* 项的权重大致相等。我们通过实验显示，结果对宽范围的 $λ$ 值不敏感(表9)。我们还注意到，上面的归一化不是必需的，可以简化。

![Faster R-CNN 表9](E:\Typora\论文笔记\Faster R-CNN 表9.png)

表9：Faster R-CNN使用方程(1)中不同的 $λ$ 值在PASCAL VOC 2007测试集上的检测结果。网络是VGG-16。训练数据是VOC 2007训练集。使用 $λ=10$ （69.9%）的默认设置与表3中的相同。

​		对于边界框回归，我们采用[5]中的4个坐标参数化：
$$
t_x=(x−x_a)/w_a,t_y=(y−y_a)/h_a,\\
t_w=log(w/w_a),t_h=log(h/h_a),\\
t^*_x=(x^*−x_a)/w_a,t^*_y=(y^*−y_a)/h_a,\\
t^*_w=log(w^*/w_a),t^*_h=log(h^*/h_a),
$$
其中，$x$，$y$，$w$ 和 $h$表示边界框的中心坐标及其宽和高。变量 $x$，$x_a$ 和$x^*$ 分别表示预测边界框，锚盒和实际边界框（类似于$y,w,h$）。这可以被认为是从锚盒到邻近的实际边界框的回归。

​		然而，**我们的方法通过与之前的基于RoI（感兴趣区域）方法[1]，[2]不同的方式来实现边界框回归**。**在[1]，[2]中，对任意大小的RoI池化的特征执行边界框回归，并且回归权重由所有区域大小共享。在我们的公式中，用于回归的特征在特征映射上具有相同的空间大小（3×3）**。为了说明不同的大小，学习一组 $k$ 个边界框回归器。每个回归器负责一个尺度和一个长宽比，而 $k$ 个回归器不共享权重。因此，由于锚点的设计，即使特征具有固定的尺度/比例，仍然可以预测各种尺寸的边界框。

##### 3.1.3 训练RPN

​		RPN可以通过反向传播和随机梯度下降（SGD）进行端对端训练[35]。我们遵循[2]的“以图像为中心”的采样策略来训练这个网络。每个小批量数据都从包含许多正面和负面示例锚点的单张图像中产生。**对所有锚点的损失函数进行优化是可能的，但是这样会偏向于负样本，因为它们是占主导地位的。取而代之的是，我们在图像中随机采样256个锚点，计算一个小批量数据的损失函数，其中采样的正锚点和负锚点的比率可达1:1。如果图像中的正样本少于128个，我们使用负样本填充小批量数据**。

​		我们通过从标准方差为0.01的零均值高斯分布中提取权重来随机初始化所有新层。所有其他层（即共享卷积层）通过预训练的ImageNet分类模型[36]来初始化，如同标准实践[5]。我们调整ZF网络的所有层，以及VGG网络的conv3_1及其之上的层以节省内存[2]。对于60k的小批量数据，我们使用0.001的学习率，对于PASCAL VOC数据集中的下一个20k小批量数据，使用0.0001。我们使用0.9的动量和0.0005的重量衰减[37]。我们的实现使用Caffe[38]。

#### 3.2 RPN 和 Fast R-CNN共享特征

​		到目前为止，我们已经描述了如何训练用于区域提议生成的网络，没有考虑将利用这些提议的基于区域的目标检测CNN。对于检测网络，我们采用Fast R-CNN[2]。接下来我们介绍一些算法，学习由RPN和Fast R-CNN组成的具有共享卷积层的统一网络（图2）。

​		独立训练的RPN和Fast R-CNN将以不同的方式修改卷积层。因此，我们需要开发一种允许在两个网络之间共享卷积层的技术，而不是学习两个独立的网络。我们讨论三个方法来训练具有共享特征的网络：

**（i）交替训练**。在这个解决方案中，我们首先训练RPN，并使用这些提议来训练Fast R-CNN。由Fast R-CNN微调的网络然后被用于初始化RPN，并且重复这个过程。这是本文所有实验中使用的解决方案。

**（ii）近似联合训练**。在这个解决方案中，RPN和Fast R-CNN网络在训练期间合并成一个网络，如图2所示。在每次SGD迭代中，前向传递生成区域提议，在训练Fast R-CNN检测器将这看作是固定的、预计算的提议。反向传播像往常一样进行，其中对于共享层，组合来自RPN损失和Fast R-CNN损失的反向传播信号。这个解决方案很容易实现。但是这个解决方案忽略了关于提议边界框的坐标（也是网络响应）的导数，因此是近似的。在我们的实验中，我们实验发现这个求解器产生了相当的结果，**与交替训练相比，训练时间减少了大约25−50%**。这个求解器包含在我们发布的Python代码中。

**（iii）非近似的联合训练**。如上所述，由RPN预测的边界框也是输入的函数。Fast R-CNN中的RoI池化层[2]接受卷积特征以及预测的边界框作为输入，所以理论上有效的反向传播求解器也应该包括关于边界框坐标的梯度。在上述近似联合训练中，这些梯度被忽略。在一个非近似的联合训练解决方案中，我们需要一个关于边界框坐标可微分的RoI池化层。这是一个重要的问题，可以通过[15]中提出的“RoI扭曲”层给出解决方案，这超出了本文的范围。

**四步交替训练。**在**本文中，我们采用实用的四步训练算法**，通过交替优化学习共享特征。在第一步中，我们按照3.1.3节的描述训练RPN。该网络使用ImageNet的预训练模型进行初始化，并针对区域提议任务进行了端到端的微调。在第二步中，我们使用由第一步RPN生成的提议，由Fast R-CNN训练单独的检测网络。该检测网络也由ImageNet的预训练模型进行初始化。此时两个网络不共享卷积层。在第三步中，我们使用检测器网络来初始化RPN训练，但是我们修正共享的卷积层，并且只对RPN特有的层进行微调。现在这两个网络共享卷积层。最后，保持共享卷积层的固定，我们对Fast R-CNN的独有层进行微调。因此，两个网络共享相同的卷积层并形成统一的网络。类似的交替训练可以运行更多的迭代，但是我们只观察到可以忽略的改进。

#### 3.3 实现细节

​		我们在单尺度图像上训练和测试区域提议和目标检测网络[1]，[2]。我们重新缩放图像，使得它们的短边是 $s=600$ 像素[2]。多尺度特征提取（使用图像金字塔）可能会提高精度，但不会表现出速度与精度的良好折衷[2]。在重新缩放的图像上，最后卷积层上的ZF和VGG网络的总步长为16个像素，因此在调整大小（〜500×375）之前，典型的PASCAL图像上的总步长为〜10个像素。即使如此大的步长也能提供良好的效果，尽管步幅更小，精度可能会进一步提高。

​		**对于锚点，我们使用了 $3$ 个尺度，边界框面积分别为 $128^2$ ，$256^2$ 和 $512^2$ 个像素，以及 $1:1$，$1:2$ 和 $2:1$的长宽比**。这些超参数不是针对特定数据集仔细选择的，我们将在下一节中提供有关其作用的消融实验。如上所述，我们的解决方案不需要图像金字塔或滤波器金字塔来预测多个尺度的区域，节省了大量的运行时间。图3（右）显示了我们的方法在广泛的尺度和长宽比方面的能力。表1显示了使用ZF网络的每个锚点学习到的平均提议大小。我们注意到，我们的算法允许预测比基础感受野更大。这样的预测不是不可能的——如果只有目标的中间部分是可见的，那么仍然可以粗略地推断出目标的范围。

![Faster R-CNN 表1](E:\Typora\论文笔记\Faster R-CNN 表1.png)

表1：使用ZF网络的每个锚点学习到的平均提议大小（$s=600$ 的数字）。

​		跨越图像边界的锚盒需要小心处理。在训练过程中，我们忽略了所有的跨界锚点，所以不会造成损失。对于一个典型的 $1000×600$ 的图片，总共将会有大约20000（$≈60×40×9$）个锚点。跨界锚点被忽略，每张图像约有6000个锚点用于训练。如果跨界异常值在训练中不被忽略，则会在目标函数中引入大的，难以纠正的误差项，且训练不会收敛。但在测试过程中，我们仍然将全卷积RPN应用于整张图像。这可能会产生跨边界的提议边界框，我们剪切到图像边界。

​		一些RPN提议互相之间高度重叠。为了减少冗余，我们在提议区域根据他们的*cls*分数采取非极大值抑制（NMS）。我们将NMS的IoU阈值固定为0.7，这就给每张图像留下了大约2000个提议区域。正如我们将要展示的那样，NMS不会损害最终的检测准确性，但会大大减少提议的数量。在NMS之后，我们使用前N个提议区域来进行检测。接下来，我们使用2000个RPN提议对Fast R-CNN进行训练，但在测试时评估不同数量的提议。

### 4. 实验

#### 4.1 PASCAL VOC上的实验

#### 4.2 在MS COCO上的实验

#### 4.3 从MS COCO到PASCAL VOC

### 5. 结论

​		我们已经提出了RPN来生成高效，准确的区域提议。通过与下游检测网络共享卷积特征，区域提议步骤几乎是零成本的。我们的方法使统一的，基于深度学习的目标检测系统能够以接近实时的帧率运行。学习到的RPN也提高了区域提议的质量，从而提高了整体的目标检测精度。